<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
    <title>AudioSpace: Generating Spatial Audio from 360-Degree Video</title>
    <style>
        body {
            font-family: 'Times New Roman', Times, serif;
            text-align: center;
            padding: 0; 
            background-color: #f7f7f7;
            width: 1000px; 
            margin: 0 auto; 
        }

        h1 {
            margin-bottom: 40px;
        }

        h2 {
            font-size: 18px;
            text-align: left;
            margin-bottom: 5px;
        }

        hr {
            border: none;
            border-top: 2px solid #6f6b6b;
            margin-top: 5px;
            margin-bottom: 5px;
            text-align: left;
        }
        #video-selector {
            width: 100%;
            max-width: 900px;
            margin: 20px auto;
            display: flex;
            flex-direction: column; /* Stack the rows vertically */
            gap: 80px; /* Increase space between video rows */
        }
        .chapter-title {
            font-family: 'Quicksand', sans-serif; 
            font-size: 18px;
            text-align: left;
            margin-bottom: 5px;
            color: #777; 
        }

        .chapter-line {
            border: none;
            border-top: 2px solid #d0d0d0; 
            margin-top: 5px;
            margin-bottom: 5px;
            text-align: left;
        }
        .section {
            background-color: #ffffff; /* White background for section */
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1); /* Light shadow for separation */
            margin-bottom: 40px; /* Add space below each section */
        }
        .section-title {
            font-size: 24px;
            color: #333;
            margin-bottom: 20px;
        }
        .video-row {
            display: flex;
            justify-content: space-between;
            flex-wrap: wrap;
            gap: 20px; /* Space between videos */
        }

        /* Each video column size, reducing the width to fit 4 per row */
        .video-column {
            width: 22%; /* Make the videos smaller to fit 4 per row */
            margin-bottom: 20px;
            display: flex;
            flex-direction: column;
            align-items: center;
        }


        .video-player-container {
            width: 100%; /* Make video take up full width of the column */
            height: 200px;
            position: relative;
        }
        .video-player-container iframe {
            width: 100%;
            height: 100%;
            border: none;
        }
        .fullscreen-btn {
            position: absolute;
            top: 10px;
            right: 10px;
            background-color: rgba(0, 0, 0, 0.6);
            color: white;
            border: none;
            padding: 5px 10px;
            cursor: pointer;
            border-radius: 5px;
        }
        .caption {
            font-size: 18px; /* Increase font size */
            color: #555;
            margin-top: 20px; /* Increase the space between caption and video */
            margin-bottom: 20px; /* Add space below caption */
            text-align: center;
            font-weight: bold;
            position: relative;
            top: 0; /* Adjust caption position */
            left: 50%;
            transform: translateX(-50%); /* Center horizontally */
        }
        .video-title {
            font-size: 16px; /* Increase font size for title */
            color: #555;
            margin-top: 20px; /* Increase space between video and title */
        }
        .image-container {
            margin: 20px auto;
            text-align: center;
        }
        .image-caption {
            font-size: 16px;
            color: #555;
            margin-top: 10px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-bottom: 30px;
        }
        caption {
            font-style: italic;
            text-align: center;
            caption-side: bottom; 
            margin-top: 10px; 
        }
        th, td {
            padding: 8px;
            text-align: center;
            border: 1px solid #ddd;
        }

        th {
            background-color: #f2f2f2;
        }

        .image-caption {
            font-style: italic;
            text-align: center;
        }
        .button-container {
            display: flex;
            flex-wrap: wrap; 
            justify-content: space-between; 
        }
        .button-container .fold-btn {
            flex: 0 0 22%;  
            margin-bottom: 10px;  
        }

        
    </style>
</head>
<body>
    <h1>AudioSpace: Generating Spatial Audio from 360-Degree Video</h1>

    <!-- Abstract section -->
    <div class="abstract" style="text-align: center;"></div>
        <h2 class="chapter-title">Abstract</h2>
        <hr class="chapter-line">
        <p style="text-align: left;"> Traditional video-to-audio generation techniques primarily focus on field-of-view (FoV) video and non-spatial audio, often missing the spatial cues necessary for accurately representing sound sources in 3D environments. To address this limitation, we introduce a novel task, <strong>360V2SA</strong>, to generate spatial audio from 360-degree videos, specifically producing First-order Ambisonics (FOA) audio - a standard format for representing 3D spatial audio that captures sound directionality and enables realistic 3D audio reproduction. We first create <strong>Sphere360</strong>, a novel dataset tailored for this task that is curated from real-world data. We also design an efficient semi-automated pipeline for collecting and cleaning paired video-audio data. To generate spatial audio from 360-degree video, we propose a novel framework <strong>AudioSpace</strong>, which leverages self-supervised pre-training using both spatial audio data (in FOA format) and large-scale non-spatial data. Furthermore, AudioSpace features a dual-branch framework that utilizes both panoramic and FoV video inputs to capture comprehensive local and global information from 360-degree videos. Experimental results demonstrate that AudioSpace achieves state-of-the-art performance across both objective and subjective metrics on Sphere360.</p>
    </div>
    
    <div class="video-carousel-section" style="text-align: center;">
        <h2 class="chapter-title">360-Degree Demo Player</h2>
        <hr class="chapter-line">
        <strong style="font-size: 19px; display: block; margin-bottom: 20px;">ðŸŽ§ Please wear headphones and drag the screen to experience the 360-degree audio ðŸŽ¥. </strong>
        <div class="carousel-container" style="display: flex; justify-content: center; align-items: center; position: relative;">
            <button id="prevBtn" onclick="moveCarousel(-1)" style="position: absolute; left: 10px;">&#8249;</button>
            <div id="carousel" style="display: flex; justify-content: center; overflow: hidden; max-width: 900px;">
                <!-- Video slots to be dynamically populated -->
            </div>
            <button id="nextBtn" onclick="moveCarousel(1)" style="position: absolute; right: 10px;">&#8250;</button>
        </div>
        <div id="video-id" class="caption" style="margin-top: 10px; font-weight: bold;"></div>
    </div>
    
    <script>
        // Array of WebM video paths for the carousel
        const videoPaths = [
            'videos_gen/P72n97ONayA_50.webm',
            'videos_gen/VEvawqEmWQo_29.webm',
            'videos_gen/Wc_gzO9i38k_15.webm',
            'videos_gen/NdE7uYVaynQl_0.webm',
        ];

        let currentIndex = 0;  

        // Create a video player container with demo-player.html iframe
        function createVideoPlayer2(videoPath, isMain) {
            const playerContainer = document.createElement('div');
            playerContainer.className = 'video-player-container';
            playerContainer.style.position = 'relative';
            
            // Add styles for the container
            playerContainer.style.border = '2px solid gray';
            playerContainer.style.padding = '10px';
            playerContainer.style.borderRadius = '10px';
            playerContainer.style.backgroundColor = '#f9f9f9';

            // Initially, use a canvas to show the first frame of the video
            const thumbnailCanvas = document.createElement('canvas');
            const thumbnailContext = thumbnailCanvas.getContext('2d');
            thumbnailCanvas.width = 320;  // Set desired width for the thumbnail
            thumbnailCanvas.height = 180; // Set desired height for the thumbnail

            const videoElement = document.createElement('video');
            videoElement.src = videoPath;
            videoElement.preload = 'metadata'; // Preload metadata to get the first frame

            videoElement.onloadedmetadata = function() {
                // Once the metadata is loaded, we can draw the first frame on the canvas
                videoElement.currentTime = 0;
                videoElement.onseeked = function() {
                    thumbnailContext.drawImage(videoElement, 0, 0, thumbnailCanvas.width, thumbnailCanvas.height);
                };
            };

            // If it's the main (center) video, create an iframe for playing
            if (isMain) {
                const iframe = document.createElement('iframe');
                iframe.src = `demo-player.html?videoPath=${encodeURIComponent(videoPath)}`;
                iframe.width = '100%';
                iframe.height = '100%';
                iframe.style.border = 'none';
                playerContainer.appendChild(iframe);
            } else {
                // For the side videos, show the first frame centered within the container
                playerContainer.style.display = 'flex';
                playerContainer.style.alignItems = 'center';  // Vertically center the thumbnail
                playerContainer.style.justifyContent = 'center';  // Horizontally center the thumbnail
                playerContainer.style.overflow = 'hidden';  // Hide any overflowing content
                playerContainer.appendChild(thumbnailCanvas);
            }

            // Remove the fullscreen button for side videos
            if (isMain) {
                const fullscreenBtn = document.createElement('button');
                fullscreenBtn.innerHTML = 'â›¶';
                fullscreenBtn.style.position = 'absolute';
                fullscreenBtn.style.top = '10px';
                fullscreenBtn.style.right = '10px';
                fullscreenBtn.style.padding = '10px';
                fullscreenBtn.style.fontSize = '20px';
                fullscreenBtn.style.backgroundColor = 'rgba(0, 0, 0, 0.7)';
                fullscreenBtn.style.color = 'white';
                fullscreenBtn.style.border = 'none';
                fullscreenBtn.style.borderRadius = '50%';
                fullscreenBtn.style.cursor = 'pointer';

                fullscreenBtn.addEventListener('click', function() {
                    if (!document.fullscreenElement) {
                        playerContainer.requestFullscreen();
                    } else {
                        document.exitFullscreen();
                    }
                });

                playerContainer.appendChild(fullscreenBtn);
            }

            return playerContainer;
        }

        // Populate the carousel with the first three videos initially
        const carousel = document.getElementById('carousel');
        const firstVideoPlayer = createVideoPlayer2(videoPaths[currentIndex], true);
        const secondVideoPlayer = createVideoPlayer2(videoPaths[(currentIndex + 1) % videoPaths.length], false);
        const thirdVideoPlayer = createVideoPlayer2(videoPaths[(currentIndex + 2) % videoPaths.length], false);
        carousel.appendChild(firstVideoPlayer);
        carousel.appendChild(secondVideoPlayer);
        carousel.appendChild(thirdVideoPlayer);

        function moveCarousel(direction) {
            // Update the current index, allowing for wrap-around
            currentIndex = (currentIndex + direction + videoPaths.length) % videoPaths.length;

            const videoWrapper = document.createElement('div');
            videoWrapper.style.display = 'flex';
            videoWrapper.style.justifyContent = 'center';
            videoWrapper.style.position = 'relative';

            const prevVideoIndex = (currentIndex - 1 + videoPaths.length) % videoPaths.length;
            const nextVideoIndex = (currentIndex + 1) % videoPaths.length;

            const prevVideoPlayer = createVideoPlayer2(videoPaths[prevVideoIndex], false);
            const currentVideoPlayer = createVideoPlayer2(videoPaths[currentIndex], true);
            const nextVideoPlayer = createVideoPlayer2(videoPaths[nextVideoIndex], false);

            // Set the current video player in the middle
            prevVideoPlayer.style.width = '35%';
            nextVideoPlayer.style.width = '35%';
            currentVideoPlayer.style.width = '100%';

            videoWrapper.appendChild(prevVideoPlayer);
            videoWrapper.appendChild(currentVideoPlayer);
            videoWrapper.appendChild(nextVideoPlayer);

            carousel.innerHTML = ''; // Clear the carousel and append the new video wrapper
            carousel.appendChild(videoWrapper);

            const videoIdElement = document.getElementById('video-id');
            videoIdElement.innerText = ``;
        }

        // Initialize the carousel
        moveCarousel(0);


    </script>
    <!-- Image section -->
    <div class="image-container">
        <h2 class="chapter-title">Motivation</h2>
        <hr class="chapter-line">
        <p style="text-align: left; font-size: 16px; max-width: 96%; color: #333;">
            The rapid advancement of virtual reality and immersive technologies has significantly amplified the demand for realistic audio-visual experiences.
            However, current video-to-audio methods typically generate non-spatial (mono or stereo) audio, which lacks essential directional information, and rely on limited perspective videos that miss crucial visual context. 
            As illustrated in Figure <a href="#fig1">1-a</a>, panoramic videos capture a complete 360-degree view, enabling the simultaneous observation of all sound sourcesâ€”such as a moving trainâ€”that remain invisible in frontal perspectives. 
            Additionally, Figure <a href="#fig2">1-b</a> shows that stereo audio fails to maintain sound localization during head rotations, whereas spatial audio in First-order Ambisonics (FOA) retains accurate positioning. 
            To address these limitations, we introduce 360V2SA, a novel task that generates FOA spatial audio directly from 360-degree videos, leveraging comprehensive visual information to enhance audio realism and immersion.
        </p>
        
        <div style="display: flex; justify-content: space-between; gap: 5px;">
            <figure id="fig1">
                <div style="flex: 1; text-align: center;">
                    <img src="image/figure1-a.png" alt="Image 1" style="width: 100%; max-width: 440px;">
                    <div class="image-caption">(a) Comparison of panoramic video and perspective video.</div>
                </div>
            </figure>
            <figure id="fig2">
                <div style="flex: 1; text-align: center;">
                    <img src="image/figure1-b.png" alt="Image 2" style="width: 100%; max-width: 440px;">
                    <div class="image-caption">(b) Comparison of stereo audio and FOA audio under head rotation.</div>
                </div>
            </figure>
        </div>
        <div class="image-caption" style="text-align: center;">Figure 1. (a) shows the scene of a moving train that appears and gradually disappears in a panoramic view without being visible in the frontal perspective. (b) compares the audio localization before and after head rotation, illustrating how stereo audio fails to maintain sound localization while spatial audio (in FOA format) retains accurate positioning.</div>
    </div>
        <h2 class="chapter-title">Architecture</h2>
        <hr class="chapter-line">
        <p style="text-align: left; font-size: 16px; color: #333;">
            As illustrated in Figure <a href="#fig3">2</a> , AudioSpace consists of two main stages: (1) we employ a coarse-to-fine self-supervised flow matching pre-training (Figure 2a) to alleviate the issue of data scarcity using both unlabeled spatial and non-spatial audio. 
            (2) In the fine-tuning stage (Figure 2b), we perform spatial-aware supervised fine-tuning by utilizing a dual-branch video representation combined with a flow matching objective.
        </p>
        <figure id="fig3">
            <img src="image/framework.png" alt="Image 2" style="width: 100%; max-width: 900px; margin-top: 20px;">
            <div class="image-caption">Figure 2. A high-level overview of AudioSpace. The model leverages stereo and FOA audios for self-supervised pre-training using token masking. AudioSpace efficiently trains for conditional generation during fine-tuning, supported by robust panoramic video representation. DiT denotes Diffusion Transformer. </div>
        </figure>
    </div>

    <div class="intro" style="text-align: center;"></div>
        <h2 class="chapter-title">Samples</h2>
        <hr class="chapter-line">
        <p style="text-align: left;">
            The following presents a comparison between the ground truth (GT), the results generated by AudioSpace, and two baseline methods (MMAudio + AS and ViSAGe), with each video providing a different perspective on the 360-degree audio experience. The video ID format is "youtube id _ start time(s)", with each video segment lasting 10 seconds. FOA audio from these videos is decoded into binaural audio using <a href="https://github.com/GoogleChrome/omnitone" target="_blank">Omnitone</a>.
            <br><br>
        </p>
    </div>

    <div id="video-selector">


    </div>
    
    <script>
        const sections = [];
    
        function generateUI() {
            const sectionContainer = document.getElementById('video-selector');
            let interval = 2;
            sectionContainer.innerHTML = "";  
    
            // Create a container for the fold buttons to be displayed in a row
            const buttonContainer = document.createElement('div');
            buttonContainer.className = 'button-container';
            sectionContainer.appendChild(buttonContainer);
    
            // Create and display buttons in a row of five
            sections.forEach((section, i) => {
                if (i % interval === 0) {
                    const foldButton = createFoldButton(i, interval);
                    buttonContainer.appendChild(foldButton);
    
                    const sectionDiv = document.createElement('div');
                    sectionDiv.className = 'sections';
                    sectionDiv.style.display = 'none'; 
    
                    foldButton.onclick = () => toggleGroupVisibility(sectionDiv, i, i + interval,interval);
    
                    sectionContainer.appendChild(sectionDiv);
                }
            });
        }
    
        function createFoldButton(startIndex, interval) {
            const foldButton = document.createElement('button');
            foldButton.className = 'fold-btn';
            foldButton.innerText = `Show/Hide Set ${Math.floor(startIndex / interval) + 1}`;
            foldButton.style.borderRadius = '20px';
            foldButton.style.padding = '10px 20px';
            foldButton.style.fontFamily = 'Arial, sans-serif';
            foldButton.style.fontWeight = 'bold';
            foldButton.style.fontSize = '16px';
            foldButton.style.backgroundColor = '#4CAF50';
            foldButton.style.color = 'white';
            foldButton.style.border = 'none';
            foldButton.style.cursor = 'pointer';
            foldButton.style.margin = '5px';
    
            foldButton.onmouseover = () => {
                if (foldButton.style.backgroundColor === 'rgb(76, 175, 80)') {  // Green color check
                    foldButton.style.backgroundColor = '#45a049';  // Hover effect
                }
            };

            foldButton.onmouseout = () => {
                if (foldButton.style.backgroundColor === 'rgb(69, 160, 73)') {  // Hovered green check
                    foldButton.style.backgroundColor = '#4CAF50';  // Reset to green
                }
            };

    
            return foldButton;
        }
    
        function toggleGroupVisibility(sectionDiv, startIndex, endIndex, interval) {
            const isHidden = sectionDiv.style.display === 'none';
            
            const foldButton = document.querySelector(`.fold-btn:nth-child(${Math.floor(startIndex / interval) + 1})`);
            
            if (isHidden) {
                sectionDiv.style.display = 'block';
                sectionDiv.innerHTML = '';  
                sectionDiv.appendChild(generateSectionContent(startIndex, endIndex));
            } else {
                sectionDiv.style.display = 'none';
            }
            foldButton.style.backgroundColor = isHidden ? '#D3D3D3' : '#4CAF50';
        }

    
        function generateSectionContent(startIndex, endIndex) {
            const sectionDiv = document.createElement('div');
            sections.slice(startIndex, endIndex).forEach((section) => {
                const sectionDivItem = document.createElement('div');
                sectionDivItem.className = 'section';
    
                const sectionTitle = document.createElement('div');
                sectionTitle.className = 'section-title';
                sectionTitle.innerText = section.title;
                sectionDivItem.appendChild(sectionTitle);
    
                section.cases.forEach((videoCase) => {
                    const videoRow = document.createElement('div');
                    videoRow.className = 'video-row';
    
                    const gtVideoPlayer = createVideoPlayer(videoCase.gt);
                    const gtTitle = document.createElement('div');
                    gtTitle.className = 'video-title';
                    gtTitle.innerText = 'Ground Truth';
    
                    const gtColumn = document.createElement('div');
                    gtColumn.className = 'video-column';
                    gtColumn.appendChild(gtVideoPlayer);
                    gtColumn.appendChild(gtTitle);
    
                    videoRow.appendChild(gtColumn);
    
                    videoCase.generated.forEach((prefix) => {
                        const generatedVideoPath = `${prefix}${videoCase.caption}.webm`;
                        const generatedVideoPlayer = createVideoPlayer(generatedVideoPath);
    
                        const generatedTitle = document.createElement('div');
                        generatedTitle.className = 'video-title';
                        generatedTitle.innerText = getGeneratedTitle(prefix);
    
                        const generatedColumn = document.createElement('div');
                        generatedColumn.className = 'video-column';
                        generatedColumn.appendChild(generatedVideoPlayer);
                        generatedColumn.appendChild(generatedTitle);
    
                        videoRow.appendChild(generatedColumn);
                    });
    
                    sectionDivItem.appendChild(videoRow);
    
                    const captionText = document.createElement('div');
                    captionText.className = 'caption';
                    captionText.innerText = 'Video ID: ' + videoCase.caption;
    
                    sectionDivItem.appendChild(captionText);
                });
    
                sectionDiv.appendChild(sectionDivItem);
            });
    
            return sectionDiv;
        }
    
        function createVideoPlayer(videoPath) {
            const playerContainer = document.createElement('div');
            playerContainer.className = 'video-player-container';
    
            const iframe = document.createElement('iframe');
            iframe.src = `demo-player.html?videoPath=${encodeURIComponent(videoPath)}`;
    
            const fullscreenButton = document.createElement('button');
            fullscreenButton.className = 'fullscreen-btn';
            fullscreenButton.innerText = 'â›¶';
            fullscreenButton.onclick = () => toggleFullscreen(playerContainer);
    
            playerContainer.appendChild(iframe);
            playerContainer.appendChild(fullscreenButton);
    
            return playerContainer;
        }
    
        function toggleFullscreen(playerContainer) {
            if (!document.fullscreenElement) {
                playerContainer.requestFullscreen().catch(err => console.log("Fullscreen failed", err));
            } else {
                document.exitFullscreen();
            }
        }
    
        function getGeneratedTitle(prefix) {
            if (prefix === 'videos_gen/') return 'AudioSpace';
            if (prefix === 'videos_mmaudio/') return 'MMAudio + AS';
            if (prefix === 'videos_visage/') return 'ViSAGe';
            return '';
        }
    
        fetch('sections.json')  
            .then(response => response.json())
            .then(jsonData => {
                for (const title in jsonData) {
                    if (jsonData.hasOwnProperty(title)) {
                        sections.push({
                            title: title,
                            cases: jsonData[title].map(caption => ({
                                gt: `videos_gt/${caption}.webm`,
                                generated: ['videos_gen/', 'videos_mmaudio/', 'videos_visage/'],
                                caption: caption
                            }))
                        });
                    }
                }
                console.log("Loaded sections:", sections);
                generateUI();  
            })
            .catch(error => console.error("Error loading JSON:", error));
    </script>
    
    


    </div>
        <h2 class="chapter-title"> Quantitative and Qualitative Results</h2>
        <hr class="chapter-line">
        <!-- Table 1: Performance Comparison -->
        <table id="table1">
            <thead>
                <tr>
                    <th>Model</th>
                    <th>Dataset</th>
                    <th>FDâ†“</th>
                    <th>KLâ†“</th>
                    <th>Î”<sub>abs</sub>Î¸â†“</th>
                    <th>Î”<sub>abs</sub>Ï†â†“</th>
                    <th>Î”<sub>Angular</sub>â†“</th>
                    <th>MOS-SQâ†‘</th>
                    <th>MOS-AFâ†‘</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>GT</td>
                    <td>YT360-Test</td>
                    <td>/</td>
                    <td>/</td>
                    <td>/</td>
                    <td>/</td>
                    <td>/</td>
                    <td>85.38Â±0.95</td>
                    <td>87.85Â±1.21</td>
                </tr>
                <tr>
                    <td>Diff-Foley + AS</td>
                    <td>YT360-Test</td>
                    <td>361.65</td>
                    <td>2.22</td>
                    <td>/</td>
                    <td>/</td>
                    <td>/</td>
                    <td>67.21Â±0.95</td>
                    <td>70.34Â±1.76</td>
                </tr>
                <tr>
                    <td>MMAudio + AS</td>
                    <td>YT360-Test</td>
                    <td>190.40</td>
                    <td>1.51</td>
                    <td>/</td>
                    <td>/</td>
                    <td>/</td>
                    <td>73.25Â±1.05</td>
                    <td>76.77Â±1.23</td>
                </tr>
                <tr>
                    <td>ViSAGe (FOV)</td>
                    <td>YT360-Test</td>
                    <td>199.09</td>
                    <td>1.86</td>
                    <td>2.21</td>
                    <td>0.88</td>
                    <td>1.99</td>
                    <td>71.82Â±1.98</td>
                    <td>72.17Â±1.47</td>
                </tr>
                <tr>
                    <td>ViSAGe (360)</td>
                    <td>YT360-Test</td>
                    <td>225.52</td>
                    <td>1.95</td>
                    <td>2.18</td>
                    <td>0.86</td>
                    <td>1.98</td>
                    <td>72.45Â±1.64</td>
                    <td>72.96Â±1.39</td>
                </tr>
                <tr>
                    <td><strong>AudioSpace</strong></td>
                    <td>YT360-Test</td>
                    <td><strong>92.57</strong></td>
                    <td><strong>1.64</strong></td>
                    <td><strong>1.27</strong></td>
                    <td><strong>1.53</strong></td>
                    <td><strong>1.27</strong></td>
                    <td><strong>80.37Â±0.91</strong></td>
                    <td><strong>83.49Â±1.01</strong></td>
                </tr>
                <tr>
                    <td>GT</td>
                    <td>Sphere360-Bench</td>
                    <td>/</td>
                    <td>/</td>
                    <td>/</td>
                    <td>/</td>
                    <td>/</td>
                    <td>88.41Â±0.79</td>
                    <td>90.12Â±1.08</td>
                </tr>
                <tr>
                    <td>Diff-Foley + AS</td>
                    <td>Sphere360-Bench</td>
                    <td>331.05</td>
                    <td>3.56</td>
                    <td>/</td>
                    <td>/</td>
                    <td>/</td>
                    <td>69.87Â±0.84</td>
                    <td>71.12Â±1.36</td>
                </tr>
                <tr>
                    <td>MMAudio + AS</td>
                    <td>Sphere360-Bench</td>
                    <td>271.15</td>
                    <td>2.39</td>
                    <td>/</td>
                    <td>/</td>
                    <td>/</td>
                    <td>75.34Â±0.99</td>
                    <td>77.56Â±1.22</td>
                </tr>
                <tr>
                    <td>ViSAGe (FOV)</td>
                    <td>Sphere360-Bench</td>
                    <td>210.87</td>
                    <td>2.90</td>
                    <td>1.51</td>
                    <td>0.71</td>
                    <td>1.49</td>
                    <td>73.45Â±1.42</td>
                    <td>74.89Â±1.71</td>
                </tr>
                <tr>
                    <td>ViSAGe (360)</td>
                    <td>Sphere360-Bench</td>
                    <td>219.66</td>
                    <td>2.96</td>
                    <td>1.52</td>
                    <td>0.74</td>
                    <td>1.51</td>
                    <td>74.12Â±1.18</td>
                    <td>75.34Â±1.03</td>
                </tr>
                <tr>
                    <td><strong>AudioSpace</strong></td>
                    <td>Sphere360-Bench</td>
                    <td><strong>88.30</strong></td>
                    <td><strong>1.58</strong></td>
                    <td><strong>1.36</strong></td>
                    <td><strong>0.52</strong></td>
                    <td><strong>1.28</strong></td>
                    <td><strong>84.67Â±1.06</strong></td>
                    <td><strong>87.23Â±0.98</strong></td>
                </tr>
            </tbody>
            <caption>Table 1. Performance comparison between AudioSpace and the baselines on the Sphere360-Bench test set and YT360 test set. We use objective metrics computing <strong>FD</strong>, <strong>KL divergence</strong>, Î”<sub>abs</sub>Î¸, Î”<sub>abs</sub>Ï†, and Î”<sub>Angular</sub> between estimated DoA and ground truth, as well as subjective metrics including MOS for spatial audio quality (<strong>MOS-SQ</strong>) and video-audio alignment faithfulness (<strong>MOS-AF</strong>). We report the mean and standard deviation for MOS-SQ and MOS-AF. <strong>+AS</strong> denotes adding an audio spatialization component. For metrics with a downward arrow (â†“), lower values represent better performance, while for metrics with an upward arrow (â†‘), higher values indicate better quality.</caption>
        </table>

        

        <figure id="fig4">
            <img src="image/figure3.png" alt="Image 3" style="width: 100%; max-width: 900px; margin-top: 20px;">
            <div class="image-caption">Figure 3. Qualitative Comparison. The first case on the left shows an agricultural machine moving behind, with the rectangular annotation indicating a decreasing trend in sound intensity in the GT audio. The second case on the right features a person playing a musical instrument. Since ViSAGe only generates 5-second audio, we concatenate the segments.</div>
        </figure>
        <figure id="fig5">
            <img src="image/appendix1.png" alt="Image 4" style="width: 100%; max-width: 900px; margin-top: 20px;">
            <div class="image-caption">Figure 4. Additional Quantitative Results. This case shows a train passing by. The rectangular annotation indicates that the audio generated by our model continues to capture the sound of the train leaving the frontal perspective, even after it has passed, while the audio generated by other models almost entirely fades once the train moves out of the frontal view.</div>
        </figure>
        <figure id="fig6">
            <img src="image/appendix2.png" alt="Image 5" style="width: 100%; max-width: 900px; margin-top: 20px;">
            <div class="image-caption">Figure 5. Additional Quantitative Results. The case on the left shows a continuous display of fireworks rising into the sky and exploding. The case on the right depicts several motorcycles chasing each other on a dirt road, with intense wind and engine sounds.</div>
        </figure>
        <figure id="fig7">
            <img src="image/appendix3.png" alt="Image 6" style="width: 100%; max-width: 900px; margin-top: 20px;">
            <div class="image-caption">Figure 6. Additional Quantitative Results. The case on the left shows a camera mounted on a boat navigating through the waves, with the bow plunging into the water and splashing onto the screen. The case on the right shows the viewpoint moving through a noisy crowd in an indoor environment.</div>
        </figure>

    </div>



</body>
</html>
